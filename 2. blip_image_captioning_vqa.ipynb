{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTJk3PJ67yMv"
      },
      "source": [
        "> ## Since Blip2ForConditionalGeneration (“Salesforce/blip2-opt-2.7b”) uses more memory, I have loaded it in later part to avoid memory issues.\n",
        "> ## Memory usage is provided for both the models before and after inference.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjBQaf11f_LK",
        "outputId": "5bae6c80-cefd-4c08-b6a3-de1adc53bfa7"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import transformers\n",
        "    print(\"Transformers is already installed.\")\n",
        "except ImportError:\n",
        "    print(\"Transformers not found. Installing...\")\n",
        "    !pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CzcW0FEpTg9"
      },
      "source": [
        "> models used\n",
        ">> Blip2ForConditionalGeneration (“Salesforce/blip2-opt-2.7b”): is used for conditional generation like, asking for cpationing image, and visual Q&A.\n",
        "\n",
        ">> Blip2ForImageTextRetrieval(\"Salesforce/blip2-itm-vit-g\"): is used for ZS text retrival for a given image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaxzH3fpfoip"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (Blip2ForImageTextRetrieval,Blip2ForConditionalGeneration,AutoProcessor, AddedToken)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmwgolV_xYZL"
      },
      "outputs": [],
      "source": [
        "def memory_stats():\n",
        "    # print(\"GPU memory Allocated: \",torch.cuda.me()/1024**2)\n",
        "    freeMem, total  = torch.cuda.mem_get_info()\n",
        "    print(f\"GPU memory Total: [{total/1024**2:.2f}] Available: [{freeMem/1024**2:.2f}] Allocated: [{torch.cuda.memory_allocated()/1024**2:.2f}] Reserved: [{torch.cuda.memory_reserved()/1024**2:.2f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kM-OSByix1WX",
        "outputId": "90a29ff3-e786-429d-d74f-f15f0e551159"
      },
      "outputs": [],
      "source": [
        "memory_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naIz1V3EgPsA",
        "outputId": "bc1ae978-b283-4f92-8afa-ec12228e01c4"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tr_model = Blip2ForImageTextRetrieval.from_pretrained(\"Salesforce/blip2-itm-vit-g\", torch_dtype=torch.float16).to(device)\n",
        "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-itm-vit-g\")\n",
        "\n",
        "# this is updated to avoid warning for deprecation for blip2 processor. ref: https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042\n",
        "processor.num_query_tokens = tr_model.config.num_query_tokens\n",
        "image_token = AddedToken(\"<image>\", normalized=False, special=True)\n",
        "processor.tokenizer.add_tokens([image_token], special_tokens=True)\n",
        "tr_model.resize_token_embeddings(len(processor.tokenizer), pad_to_multiple_of=64) # pad for efficient computation\n",
        "tr_model.config.image_token_index = len(processor.tokenizer) - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXyNep-0px7T"
      },
      "source": [
        ">  Blip2 conditional generation model have size around 15gb which is capabity of T4 GPU on colab so,we first load Blip2ForImageTextRetrieval only and measured memory usage.\n",
        "> on this run it uses around 2446Mib before inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikdJ5at9pq6r",
        "outputId": "d9a11b9f-8a02-49cb-810f-90e8abdaace9"
      },
      "outputs": [],
      "source": [
        "memory_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLt1S7CxlChY"
      },
      "source": [
        "> Download image to test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "AVRB-4D4iQkG",
        "outputId": "e7eb5341-65cc-412f-c9ee-c514db186834"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "plt.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-9ssW0bjQCQ"
      },
      "outputs": [],
      "source": [
        "possible_texts = [\"a photo of a cat\",\n",
        "         \"a photo of a dog\",\n",
        "         \"a photo of two cats\",\n",
        "         \"a photo of two cats sleeping on a pink blanket\",\n",
        "         \"a photo of two remote control on a pink blanket\",\n",
        "         \"a photo of two pink sofa\",\n",
        "         \"a photo of pink bed\",\n",
        "         \"a photo of two dogs sleeping on pink blanket\",\n",
        "         \"a photo of cats playing with remote control\",\n",
        "         \"a photo of remote controlled cat toys\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGHlxUn1kSDi"
      },
      "outputs": [],
      "source": [
        "inputs = processor(images=image, text=possible_texts, return_tensors=\"pt\", padding=True).to(device, torch.float16) # added padding to true, to match all text length, else it will throw an error.\n",
        "itc_out = tr_model(**inputs, use_image_text_matching_head=False)\n",
        "logits_per_image = itc_out.logits_per_image  # this is the image-text similarity score\n",
        "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8zZkUejkXwF",
        "outputId": "8c3a063e-bcee-41fb-d08a-ab10a2d0bd63"
      },
      "outputs": [],
      "source": [
        "max_prob_index = probs[0].argmax()\n",
        "for idx,text in enumerate(possible_texts):\n",
        "  print_statement = f\"[{probs[0][idx]:.1%}] that image is of '{text}'\"\n",
        "  if max_prob_index == idx:\n",
        "    print_statement  = f\"\\n{print_statement} <=== [BEST MATCH]\\n\"\n",
        "  print(print_statement)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ExEjv4Gs9-4"
      },
      "source": [
        "> after inference, the memory usage is around 3138Mib\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOb0nvWZs6Ib",
        "outputId": "64cb0b4a-3101-4f4b-a77b-f537d948cc7e"
      },
      "outputs": [],
      "source": [
        "memory_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAq_yjIaqBJ-"
      },
      "source": [
        "> Clean GPU memory to use other model in current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDBOH5XhqQun"
      },
      "outputs": [],
      "source": [
        "del tr_model\n",
        "del processor\n",
        "del inputs\n",
        "del itc_out\n",
        "del logits_per_image\n",
        "del probs\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ebr3AuO0wBso",
        "outputId": "ffec4721-0fbe-41bb-8c0d-130898bb6a5f"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrdC0zMnk9px"
      },
      "source": [
        "> Now Load Conditional Generation Model.\n",
        "> Let's check memory usage before loading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaJpUwq3lNxe",
        "outputId": "0e856fdb-c44d-4fb5-942a-b908c512d9a1"
      },
      "outputs": [],
      "source": [
        "memory_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9da67f8860ad47278fe0c797596f9b28",
            "1428f4940009424da4550cb876ba0dc9",
            "e3d7bd47673a4412bfde95b732c076e0",
            "c5321cb8d48c46418f9d60cb03837421",
            "e3aed630c29e4ea3a5f1053562b69658",
            "0383ddf903734f62989ca55103184f74",
            "699c7fe00ba842bdb8b270c7016ae160",
            "e7c01510b150484899849b22e01bbc1a",
            "df832f26b87d41938f9fc6c290af6fb7",
            "63f15202ca3749bd9c1f2da2d989552d",
            "2ae49d7ad9414525b724bead38eba88e"
          ]
        },
        "id": "eajqShYck8yH",
        "outputId": "bb1ecac4-e11f-445c-a015-4f34c5c761cd"
      },
      "outputs": [],
      "source": [
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
        "\n",
        "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "cgen_model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", device_map=\"auto\", torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "zKgHe42F5eWv",
        "outputId": "ae41677a-172b-49e7-db3c-d2378399f6e1"
      },
      "outputs": [],
      "source": [
        "img_url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
        "plt.imshow(raw_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHVahbXtthF0"
      },
      "source": [
        "> check memory usage for Blip conditional generation model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WgJ-5BTtovN",
        "outputId": "cf47c37c-3b56-4662-c348-b869b825ba5f"
      },
      "outputs": [],
      "source": [
        "memory_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjHzfbEyzNxs"
      },
      "source": [
        "> Inference common function to test on various prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjBAUA16qYKl"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def InferBlip(cgen_model, processor, image, question, min_length=16, max_length=64,temperature=0.0,repetition_penalty=1.3):\n",
        "  inputs = processor(images=image, text=question, return_tensors=\"pt\").to(device=\"cuda\", dtype=torch.float16)\n",
        "  do_sample = False\n",
        "  if temperature > 0:\n",
        "    do_sample = True\n",
        "\n",
        "  if not do_sample:\n",
        "    generated_ids = cgen_model.generate(**inputs, min_length=min_length,repetition_penalty=repetition_penalty,do_sample=do_sample,max_new_tokens=max_length)\n",
        "  else:\n",
        "    generated_ids = cgen_model.generate(**inputs, min_length=min_length,repetition_penalty=repetition_penalty,do_sample=do_sample, temperature=temperature,max_new_tokens=max_length)\n",
        "\n",
        "  generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "  return generated_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9dq5K5_wsiL"
      },
      "source": [
        "> lets get captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rjjBAhKwwOt",
        "outputId": "114bb4dd-5ae1-4821-9bab-268f19553b3b"
      },
      "outputs": [],
      "source": [
        "prompt = \"Q: Provide a long caption for the provided image.\\nAns:\"\n",
        "generated_text = InferBlip(cgen_model, processor, raw_image, prompt, min_length=5,max_length=64, temperature=0.0)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxPxdyid1gDv",
        "outputId": "4e4c39d5-332d-42fd-e4c4-a2c5275b6b89"
      },
      "outputs": [],
      "source": [
        "prompt = \"Q: Provide a short caption for the provided image.\\nAns:\"\n",
        "generated_text = InferBlip(cgen_model, processor, raw_image, prompt, min_length=5,max_length=30, temperature=0.0) # change max length to rtetrict model to generate short caption.\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxN-62is5rJ8"
      },
      "source": [
        "> Memory usage after inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pele0emI5qls",
        "outputId": "cfe7cabc-c1de-4e50-ae5c-a0431dfae4be"
      },
      "outputs": [],
      "source": [
        "memory_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcWGa8Uft5IT"
      },
      "source": [
        ">> Fail cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgv2-Ye7uQe_",
        "outputId": "29fdbe43-c7b1-4e93-f55c-0bf219c4e545"
      },
      "outputs": [],
      "source": [
        "prompt = \"Q: generate caption for the provided image Answer:\"  # here if we dont provide \".\" at the end of question it fails to answer\n",
        "generated_text = InferBlip(cgen_model, processor, raw_image, prompt)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiBL9TT-ryTq",
        "outputId": "a507ff90-38d0-45d9-fcf1-eaa07838703f"
      },
      "outputs": [],
      "source": [
        "prompt = \"Q: Can you please generate caption for the provided image? Answer:\"  # bias towrds yes/no questionb\n",
        "generated_text = InferBlip(cgen_model, processor, raw_image, prompt)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WASrQ6Ut61q",
        "outputId": "080b3908-ca38-4322-87f7-25df0b1c0542"
      },
      "outputs": [],
      "source": [
        "prompt = \"Q: please generate detailed long description for the provided image. Answer:\"  # very short answer despite asking for detailed description.\n",
        "generated_text = InferBlip(cgen_model, processor, raw_image, prompt, max_length=120)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlFPeYc8wKGx",
        "outputId": "b1855b07-8fd3-4d0e-c6c0-3385425d9e88"
      },
      "outputs": [],
      "source": [
        "prompt = \"Q: what is the color of the remotes in the image? Answer:\"   # color is not correct\n",
        "generated_text = InferBlip(cgen_model, processor, raw_image, prompt)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb2NW1NHwcva",
        "outputId": "2fa9ae14-a3f0-4bf9-db26-9c1b4a62aa0e"
      },
      "outputs": [],
      "source": [
        "prompt = \"Q: How many legs the cat on the left have? Answer:\" # wrong count for the object attributes\n",
        "generated_text = InferBlip(cgen_model, processor, raw_image, prompt)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_eoLDjqugCh"
      },
      "source": [
        "> working success prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwMYFqrbuiQv",
        "outputId": "50a78ce8-84a3-49a9-e81f-8f6248ff7b08"
      },
      "outputs": [],
      "source": [
        "prompt = \"Q: generate caption for the provided image. Answer:\"  # here if we provide \".\" at the end of question the it answers.\n",
        "generated_text = InferBlip(cgen_model, processor, raw_image, prompt)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sj9x7wTZuseB",
        "outputId": "e5a2fa49-d57a-4a9c-d7a7-f7f2e36b0f58"
      },
      "outputs": [],
      "source": [
        "prompt = \"Q: how many cats are there in the image? Answer:\"\n",
        "generated_text = InferBlip(cgen_model, processor, raw_image, prompt)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rSR60_Fv7Q2",
        "outputId": "0ea71197-428d-4430-a218-3dd41de7e9aa"
      },
      "outputs": [],
      "source": [
        "prompt = \"Q: how many remotes are there in the image? Answer:\"\n",
        "generated_text = InferBlip(cgen_model, processor, raw_image, prompt)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2PulImKwAIf",
        "outputId": "9cfdea63-51c0-48ed-ceb3-c2efaf02c170"
      },
      "outputs": [],
      "source": [
        "prompt = \"Q: how many remotes are there in the image? Answer:\"\n",
        "generated_text = InferBlip(cgen_model, processor, raw_image, prompt)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RryLgvHW0Iyu"
      },
      "source": [
        "> finish the sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdtt6l_Rzwn_",
        "outputId": "7bceae39-a046-424d-f620-9f8d4c872d2f"
      },
      "outputs": [],
      "source": [
        "prompt = \"two cats sleeping on a couch with remotes and television remote control in foreground, background is \"\n",
        "generated_text = InferBlip(cgen_model, processor, raw_image, prompt,  min_length=64)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDn6DTfN37lY"
      },
      "source": [
        "> Inference on Custom Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "LPxlOv7C3_GN",
        "outputId": "bd997cc0-cd3d-4994-d479-c36d032418a6"
      },
      "outputs": [],
      "source": [
        "img_url = 'http://farm4.static.flickr.com/3488/4051378654_238ca94313.jpg'\n",
        "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
        "\n",
        "plt.imshow(raw_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CROtTvtZ5MPv",
        "outputId": "ebfc88a8-3228-44cc-cb36-76bf14867785"
      },
      "outputs": [],
      "source": [
        "prompt = \"Q: Provide a short caption for the provided image.\\nAns:\"\n",
        "generated_text = InferBlip(cgen_model, processor, raw_image, prompt, min_length=5,max_length=30) # change max length to rtetrict model to generate short caption.\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_t2eau85Q0y",
        "outputId": "51276f83-4c8b-4bf2-9cf3-e051710523b8"
      },
      "outputs": [],
      "source": [
        "prompt = \"Q: How many birds are there in the provided image.\\nAns:\"\n",
        "generated_text = InferBlip(cgen_model, processor, raw_image, prompt, min_length=5,max_length=30) # change max length to rtetrict model to generate short caption.\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zc718X3_Eyw",
        "outputId": "17abe53a-afdf-424e-cfc8-f13e32e8cea0"
      },
      "outputs": [],
      "source": [
        "prompt = \"Q: is the bird fying in the provided image.\\nAns:\"\n",
        "generated_text = InferBlip(cgen_model, processor, raw_image, prompt, min_length=5,max_length=30) # change max length to rtetrict model to generate short caption.\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrGpEdUk_J_J",
        "outputId": "aee41a04-9208-4fdc-f687-82c27c659922"
      },
      "outputs": [],
      "source": [
        "prompt = \"Q: is the bird standing on a rock in the provided image.\\nAns:\"\n",
        "generated_text = InferBlip(cgen_model, processor, raw_image, prompt, min_length=5,max_length=30) # change max length to rtetrict model to generate short caption.\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT4mTytG_RQR",
        "outputId": "b554fd53-a7d2-44af-cff6-46264058c6d0"
      },
      "outputs": [],
      "source": [
        "prompt = \"Q: what do you see in the provided image.\\nAns:\"\n",
        "generated_text = InferBlip(cgen_model, processor, raw_image, prompt, min_length=5,max_length=30) # change max length to rtetrict model to generate short caption.\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz9IOlJi_Xj9",
        "outputId": "b2a681ab-4c48-4953-ad73-d313891bb8cc"
      },
      "outputs": [],
      "source": [
        "prompt = \"Q: what color is the bird?\\nAns:\"\n",
        "generated_text = InferBlip(cgen_model, processor, raw_image, prompt, min_length=5,max_length=30) # change max length to rtetrict model to generate short caption.\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xrek9_BP_cMZ",
        "outputId": "f3fd4c83-fa4e-43ea-a845-c3f17a790c87"
      },
      "outputs": [],
      "source": [
        "prompt = \"Q: can you see a nest around the bird?\\nAns:\"\n",
        "generated_text = InferBlip(cgen_model, processor, raw_image, prompt, min_length=5,max_length=30) # change max length to rtetrict model to generate short caption.\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9Q-zXDk_7KV",
        "outputId": "b01de397-946d-4912-adf1-5807c8132ee5"
      },
      "outputs": [],
      "source": [
        "prompt = \"Q: what food the bird is eating?\\nAns:\"\n",
        "generated_text = InferBlip(cgen_model, processor, raw_image, prompt, min_length=5,max_length=30) # change max length to rtetrict model to generate short caption.\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHHYC5qBAJtn",
        "outputId": "4a3a45db-6c33-4b35-cbb4-b9ebb65cf82b"
      },
      "outputs": [],
      "source": [
        "prompt = \"Q: how many baby birds are there in the image?\\nAns:\"\n",
        "generated_text = InferBlip(cgen_model, processor, raw_image, prompt, min_length=5,max_length=30) # change max length to rtetrict model to generate short caption.\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwSOlemAAQmP",
        "outputId": "fa577209-f376-4118-8047-651d56491ee0"
      },
      "outputs": [],
      "source": [
        "prompt = \"Q: Do you see bird eggs in the image ?\\nAns:\"\n",
        "generated_text = InferBlip(cgen_model, processor, raw_image, prompt, min_length=5,max_length=30) # change max length to rtetrict model to generate short caption.\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Y_AM0Qyu5i"
      },
      "source": [
        "## potential limitations\n",
        "\n",
        "1. \".\" or \"?\" at the end of question is neccesary else instructions are not being followed, so better tokenizer is needed to understand the insturctions.\n",
        "2. Model fails to understand object attributes (like number of legs, or color of object).\n",
        "3. Long captions results in hallucination due to low confidence next token prediction.\n",
        "\n",
        "\n",
        "## model capabilities\n",
        "\n",
        "1. Excellent for zero shot image captioning\n",
        "2. Good for short Q&A, answers yes no or count based question, should be really good for tasks dealing with yes/no(validation) or count for specific objects.\n",
        "\n",
        "\n",
        "## possible tweaks\n",
        "\n",
        "1. short caption can be improved for asking to continue or fill the blanks kind of questions by passing generated text back to the model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5CzcW0FEpTg9",
        "g_eoLDjqugCh"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0383ddf903734f62989ca55103184f74": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1428f4940009424da4550cb876ba0dc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0383ddf903734f62989ca55103184f74",
            "placeholder": "​",
            "style": "IPY_MODEL_699c7fe00ba842bdb8b270c7016ae160",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2ae49d7ad9414525b724bead38eba88e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63f15202ca3749bd9c1f2da2d989552d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "699c7fe00ba842bdb8b270c7016ae160": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9da67f8860ad47278fe0c797596f9b28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1428f4940009424da4550cb876ba0dc9",
              "IPY_MODEL_e3d7bd47673a4412bfde95b732c076e0",
              "IPY_MODEL_c5321cb8d48c46418f9d60cb03837421"
            ],
            "layout": "IPY_MODEL_e3aed630c29e4ea3a5f1053562b69658"
          }
        },
        "c5321cb8d48c46418f9d60cb03837421": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63f15202ca3749bd9c1f2da2d989552d",
            "placeholder": "​",
            "style": "IPY_MODEL_2ae49d7ad9414525b724bead38eba88e",
            "value": " 2/2 [01:28&lt;00:00, 41.69s/it]"
          }
        },
        "df832f26b87d41938f9fc6c290af6fb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3aed630c29e4ea3a5f1053562b69658": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3d7bd47673a4412bfde95b732c076e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7c01510b150484899849b22e01bbc1a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df832f26b87d41938f9fc6c290af6fb7",
            "value": 2
          }
        },
        "e7c01510b150484899849b22e01bbc1a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
