{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XznWzNR_CwgD"
      },
      "source": [
        "## Introduction to PyTorch, HuggingFace, and Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FenD9_ml30R3"
      },
      "source": [
        "## 1. Check installed packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCSbA6YO5pom",
        "outputId": "5ac31a12-5713-4992-adf9-1118a053e673"
      },
      "outputs": [],
      "source": [
        "# show all installed packages\n",
        "!pip list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv8BMBmQ4IJX"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGX_ZMTH3bUt",
        "outputId": "096442a3-343c-42eb-f9cb-1c233d0aacd4"
      },
      "outputs": [],
      "source": [
        "# we can also check if a specific package is installed or not by using pip\n",
        "!pip list | grep -i torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6caZpDHs5tkj"
      },
      "source": [
        "## 2. Load CLIP model to GPU and Report GPU memory usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhYPTyZW4UdO",
        "outputId": "78097690-b3df-4017-818d-c1f3c6906dfd"
      },
      "outputs": [],
      "source": [
        "# we require transformers to load clip model, let's see if it's installed.\n",
        "!pip list | grep -i transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQgTnJNa6HS1"
      },
      "source": [
        "## Check GPU memory usage before loading the model (see the usage at xxxMiB / 15360MiB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfkefomZ6E-q",
        "outputId": "21a35cb3-febd-4aa6-8b67-0e534046f320"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozypsGWp6iUB"
      },
      "source": [
        "## Load the model to GPU (Make sure to use GPU runtime)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDnYcXaN4qO9",
        "outputId": "c4d947cb-cce9-44fe-89c0-c70e147eea3c"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", device_map=device)\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", device_map=device)\n",
        "print(\"Model loaded to \",device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH2iMZtM6pD2"
      },
      "source": [
        "## check GPU usage after loading the CLIP model. (see the usage at xxxMiB / 15360MiB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1koDig_86tPi",
        "outputId": "64625484-c3ee-4c4c-8657-9a2ef5775249"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH_OX_hE7gWC"
      },
      "source": [
        "# Running CLIP model on sample image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqzAi-Mv7e5s"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7HbmlNg7OpZ"
      },
      "outputs": [],
      "source": [
        "# We require requests to fetch Image from the URL, PIL to load the image and matplotlib to show an image\n",
        "import requests\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GxWk5cz-zqU"
      },
      "outputs": [],
      "source": [
        "# lets make a inference image function because we will be reusing this code for multiple inferences.\n",
        "\n",
        "def infer_model(processor, model, image, captions, device=\"cpu\", isImageURL=False, isImagePath=False):\n",
        "  if isImageURL:\n",
        "    image = Image.open(requests.get(image, stream=True).raw)\n",
        "  elif isImagePath:\n",
        "    image = Image.open(image)\n",
        "\n",
        "  # pass two sample captions for the input image to check its similarity\n",
        "  inputs = processor(text=captions, images=image, return_tensors=\"pt\", padding=True)\n",
        "  inputs.to(device)\n",
        "\n",
        "  # infer the model\n",
        "  with torch.no_grad():\n",
        "    with torch.autocast(device):\n",
        "      outputs = model(**inputs)\n",
        "\n",
        "  logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
        "  probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
        "\n",
        "  return logits_per_image, probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "DlHS_fhd76WQ",
        "outputId": "ae10d30c-0c37-4db4-8bad-ee72e6fb25c2"
      },
      "outputs": [],
      "source": [
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "plt.imshow(image)\n",
        "\n",
        "# List the captions to test against the image\n",
        "captions = [ \"a photo of a cat\", \"a photo of a dog\"]\n",
        "\n",
        "logits_per_image, probs = infer_model(processor, model, url, captions, device, isImageURL=True)\n",
        "\n",
        "# print probs of each caption\n",
        "for i,cap in enumerate(captions):\n",
        "  print(f\"{cap}: {probs[0][i].item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjzY5wfj9eJo"
      },
      "source": [
        "# Now lets upload a sample image from our local machine to drive and run the inference on that"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmmsNAz6F6dj",
        "outputId": "0f5530bf-a47a-4597-8bab-1a7d8c54504a"
      },
      "outputs": [],
      "source": [
        "# follow the connect to drive popup instructions\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbv2386YCO2c"
      },
      "source": [
        "# uploading personcakefridge.png image to drive (attached with this notebook)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "mdoP9m7i-JFv",
        "outputId": "2359e2af-631a-42fb-a37c-2088b01910b1"
      },
      "outputs": [],
      "source": [
        "import os # to join the path\n",
        "IMAGE_FILE_NAME = \"personcakefridge.png\" # change if you are using some other image.\n",
        "image = Image.open(os.path.join(\"/content/drive/MyDrive\", IMAGE_FILE_NAME))\n",
        "plt.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCTOGDnm-azP",
        "outputId": "92db8e5e-ed3f-4d76-aa8f-29c2589dd384"
      },
      "outputs": [],
      "source": [
        "# create 5 sample captions for this image\n",
        "captions = [\n",
        "    \"a photo of a person holding a baby\",\n",
        "    \"a photo of cake on the table\",\n",
        "    \"a photo of a family celebrating birthday of a little girl\",\n",
        "    \"a photo of a person standing in front of a fridge\",\n",
        "    \"a photo of a person cutting the cake\"\n",
        "]\n",
        "\n",
        "# get inference for given image and captions\n",
        "logits_per_image, probs = infer_model(processor, model, image, captions, device, isImageURL=False, isImagePath=False)\n",
        "\n",
        "# print probs of each caption\n",
        "for i,cap in enumerate(captions):\n",
        "  print(f\"{cap}: [{probs[0][i].item():.4f}]\")\n",
        "\n",
        "# get best caption\n",
        "best_caption_idx = probs[0].argmax().item()\n",
        "best_caption = captions[best_caption_idx]\n",
        "print(f\"\\n\\n\\n Best caption: {best_caption} [{probs[0][best_caption_idx]:.4f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEjfr1po4Hhp"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
